<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose
      Tracking of Hand-Object Interaction
    </title>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"
      integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l"
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"
    />
    <style>
      body {
        font-family: "Rubik", sans-serif;
      }

      h2 {
        font-size: 40px;
        font-weight: 300;
        letter-spacing: -1px;
        margin-bottom: 1rem;
      }

      h3,
      h4 {
        margin-bottom: 1rem;
      }

      video {
        width: 100%;
        height: 100%;
      }

      code {
        background-color: #f5f5f5;
        display: block;
        padding: 20px;
        white-space: pre-wrap;
      }

      .container {
        padding: 40px 15px;
      }

      .center {
        text-align: center;
      }

      .underline {
        text-decoration: underline;
      }

      .nowrap {
        white-space: nowrap;
      }

      .authors {
        line-height: 2;
        font-size: 18px;
      }

      .section {
        padding: 10px 0 30px;
      }

      .content {
        padding: 10px 0;
      }

      .video {
        padding: 20px 0;
      }

      .image {
        padding: 0 30px;
      }

      .image-doc {
        padding: 0 25px;
      }

      .pipeline {
        width: 100%;
        height: auto;
        padding: 0 15px;
        max-width: 100%;
        box-sizing: border-box;
      }

      .pipeline img {
        width: 100%;
        height: auto;
        display: block;
        margin: 0 auto;
      }

      .embedded-video {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 56.25%;
        overflow: hidden;
        border-radius: 10px !important;
      }

      .embedded-video iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
      }
    </style>
  </head>

  <body>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <div class="section">
            <div class="content">
              <h2 class="center">
                HO-Cap: A Capture System and Dataset for 3D Reconstruction and
                Pose Tracking of Hand-Object Interaction
              </h2>
              <div class="center authors">
                <a href="" target="_blank"
                  ><span class="nowrap">Jikai Wang<sup>1</sup></span></a
                >
                &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="" target="_blank"
                  ><span class="nowrap">Qifan Zhang<sup>1</sup></span></a
                >
                &nbsp;&nbsp;&nbsp;&nbsp;
                <a
                  href="https://research.nvidia.com/person/yu-wei-chao"
                  target="_blank"
                  ><span class="nowrap">Yu-Wei Chao<sup>2</sup></span></a
                >
                &nbsp;&nbsp;&nbsp;&nbsp;
                <a
                  href="https://research.nvidia.com/person/bowen-wen"
                  target="_blank"
                  ><span class="nowrap">Bowen Wen<sup>2</sup></span></a
                >
                &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://profiles.utdallas.edu/xguo" target="_blank"
                  ><span class="nowrap">Xiaohu Guo<sup>1</sup></span></a
                >
                &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://yuxng.github.io" target="_blank"
                  ><span class="nowrap">Yu Xiang<sup>1</sup></span></a
                >
              </div>
              <div class="center authors">
                <span
                  ><sup>1</sup>University of Texas at Dallas,
                  <sup>2</sup>NVIDIA</span
                >
              </div>
            </div>
          </div>

          <div class="section">
            <div class="video">
              <video autoplay muted loop playsinline>
                <source src="assets/videos/hocap-demo.mp4" type="video/mp4" />
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <div class="section">
            <h3>Abstract</h3>
            <p>
              We introduce a data capture system and a new dataset, HO-Cap, for
              3D reconstruction and pose tracking of hands and objects in
              videos. The system leverages multiple RGB-D cameras and a HoloLens
              headset for data collection, avoiding the use of expensive 3D
              scanners or mocap systems. We propose a semi-automatic method for
              annotating the shape and pose of hands and objects in the
              collected videos, significantly reducing the annotation time
              compared to manual labeling. With this system, we captured a video
              dataset of humans interacting with objects to perform various
              tasks, including simple pick-and-place actions, handovers between
              hands, and using objects according to their affordance, which can
              serve as human demonstrations for research in embodied AI and
              robot manipulation. Our data capture setup and annotation
              framework will be available for the community to use in
              reconstructing 3D shapes of objects and human hands and tracking
              their poses in videos.
            </p>
          </div>

          <div class="section">
            <h3>Data Capture Setup (9 RGB-D Cameras + HoloLens, No Mo-cap)</h3>
            <div class="center pipeline">
              <img src="./assets/images/hardware.png" alt="Hardware Image" />
            </div>
          </div>

          <div class="section">
            <h3>Object Shape Reconstruction using a Single Azure Camera</h3>
            <div class="center pipeline">
              <img src="./assets/images/object_shape.png" alt="Object Shape" />
            </div>
            <br />
            <div class="center pipeline">
              <img
                src="./assets/images/object-shapes.png"
                alt="Object Shapes"
              />
            </div>
          </div>

          <div class="section">
            <h3>Semi-automatic Annotation Pipeline for Hand-Object Poses</h3>
            <p>
              The only human annotation required is to manually prompt two
              points for each object in the first frame to generate an initial
              segmentation mask of the object using SAM, and label the name of
              the object to associate it to an object in our database.
            </p>
            <div class="center pipeline">
              <img src="./assets/images/pipeline.png" alt="Pipeline Image" />
            </div>
          </div>

          <div class="section">
            <h3>Labels provided by HOCap Dataset</h3>
            <div class="center pipeline">
              <img src="./assets/images/vis_labels.png" alt="vis_labels Image" />
            </div>
          </div>

          <div class="section">
            <h3>Paper & Document</h3>
            <div class="row content">
              <div class="center image-doc">
                <a href="https://arxiv.org/abs/2406.06843" target="_blank">
                  <img
                    src="./assets/images/arxiv.png"
                    height="176px"
                    width="136px"
                    alt="arXiv"
                  />
                  <br />
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>arXiv</span>
                </a>
              </div>
            </div>
            <br />
            <div class="content">
              <h4>Citing HO-Cap</h4>
              <p>Please cite HO-Cap if it helps your research:</p>
              <pre><code>@misc{wang2024hocapcapturedataset3d,
      title={HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object Interaction},
      author={Jikai Wang and Qifan Zhang and Yu-Wei Chao and Bowen Wen and Xiaohu Guo and Yu Xiang},
      year={2024},
      eprint={2406.06843},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.06843},
}</code></pre>
            </div>
          </div>

          <div class="section">
            <h3>Data</h3>
            <p>
              HO-Cap is licensed under
              <span class="underline"
                ><a
                  href="https://creativecommons.org/licenses/by/4.0/"
                  target="_blank"
                  rel="noopener noreferrer"
                  >CC BY 4.0</a
                ></span
              >.
            </p>
            <p>
              The links to purchase the objects are provided in the
              <a
                href="https://utdallas.box.com/s/jrj1s2wn46cv4gvpcstbhuus15sjrw7y"
                target="_blank"
                rel="noopener noreferrer"
                ><span class="icon"><i class="fas fa-file-excel"></i></span>
                objects_info.xlsx</a
              >
              file.
            </p>
            <p>
              The rendered video of poses and segmentations for each sequence
              could be found in
              <a
                href="https://utdallas.box.com/s/4jf5cfodt7ob1arlff0zoix02mb7srov"
                target="_blank"
                rel="noopener noreferrer"
                ><span class="icon"><i class="fas fa-folder"></i></span>
                renderings</a
              >
            </p>
            <p>We provide two options for downloading the dataset:</p>
            <ol>
              <li>
                Download the data using a Python script as introduced by
                <a
                  href="https://github.com/IRVLUTD/HO-Cap?tab=readme-ov-file#download-the-ho-cap-dataset"
                  target="_blank"
                  rel="noopener noreferrer"
                  >HOCap-Toolkit</a
                >.
              </li>
              <li>
                Download the individual zipped data from Box manually:
                <ul>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/nlp4c6vtd0n8o0entxlh1vxdpcdeh0h8"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      calibration.zip (19.6 KB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/con44iqej33weg9f3rpxof61eh3x2x21"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      models.zip (52.5 MB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/w0voy9bixtxyclo52841xyamock2lxpt"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_1.zip (8.0 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/j498kxxrkvaf674tvmt4su4ad0bz9s9f"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_2.zip (20.6 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/shklq33yaoozh9gm681nxwnq0o3y0y1d"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_3.zip (18 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/dew68k7b3ya09t40818gpfxm95oa4yeq"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_4.zip (8.3 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/mutor2a09kudze1yw173gsfetsru7ces"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_5.zip (3.2 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/iyja7rdbjx2ksgjhmdu6mx3zqvaurdni"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_6.zip (14.9 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/4g5qyig6i4uz1rgrzkcu9n4mhdjs74m2"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_7.zip (9.6 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/khrb5guy8rdwnoqi4euk2w0mk5lslxkn"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_8.zip (6.2 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/3x5yitydmbmwolq9bty5dd2udu5v52fc"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      subject_9.zip (11.7 GB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/2lofbp2yd005d8o213ns77mdrtxg8eep"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      poses.zip (23.8 MB)</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://utdallas.box.com/s/ayd4st2wo588z2yqbuxalptxnz2qxlj5"
                      target="_blank"
                      rel="noopener noreferrer"
                      ><span class="icon"><i class="fas fa-download"></i></span>
                      labels.zip (1.5 GB)</a
                    >
                  </li>
                </ul>
              </li>
            </ol>
            <p>
              Once you successfully download the zip files, extract them to the
              "./datasets" folder, the directory structure should look like the
              following:
            </p>
            <pre>
<code>datasets
  ├── calibration
  ├── models
  ├── subject_1
  │   ├── 20231025_165502
  │   │   ├── 037522251142
  │   │   │   ├── color_000000.jpg
  │   │   │   ├── depth_000000.png
  │   │   │   ├── label_000000.npz
  │   │   │   └── ...
  │   │   ├── 043422252387
  │   │   ├── ...
  │   │   ├── hololens_kv5h72
  │   │   ├── meta.yaml
  │   │   ├── poses_m.npy
  │   │   ├── poses_o.npy
  │   │   └── poses_pv.npy
  │   ├── 20231025_165502
  │   └── ...
  ├── ...
  └── subject_9</code>
            </pre>
            <p>
              We also provide the recordings used for objects reconstruction in
              <a
                href="https://utdallas.box.com/s/w1yw5ucjejpc9uijk0l1catswfg1b3g2"
                target="_blank"
                rel="noopener noreferrer"
                ><span class="icon"><i class="fas fa-folder"></i></span>
                object_collections</a
              >
            </p>
            <p>
              For instructions about using the dataset please see
              <span
                ><a
                  href="https://github.com/IRVLUTD/HO-Cap"
                  target="_blank"
                  rel="noopener noreferrer"
                  >HOCap-Toolkit</a
                ></span
              >.
            </p>
          </div>

          <div class="section">
            <h3>Code</h3>
            <div class="row content">
              <div class="center image">
                <a href="https://github.com/IRVLUTD/HO-Cap" target="_blank"
                  ><img
                    src="./assets/images/github-mark.png"
                    height="64px"
                    width="64px"
                    alt="GitHub"
                /></a>
              </div>
              <div>
                <span
                  ><a href="https://github.com/IRVLUTD/HO-Cap" target="_blank"
                    >HOCap-Toolkit</a
                  ></span
                >
                <br />
                <span
                  >A Python package that provides evaluation and visualization
                  tools for the HO-Cap dataset.</span
                >
              </div>
            </div>
          </div>

          <div class="section">
            <h3>Contact</h3>
            <p>
              Send any comments or questions to Jikai Wang:
              <a href="mailto:jikai.wang@utdallas.edu"
                >jikai.wang@utdallas.edu</a
              >.
            </p>
          </div>

          <div class="section">
            <h3>Acknowledgements</h3>
            <p>
              This work was supported in part by the DARPA Perceptually enabled
              Task Guidance (PTG) Program under contract number HR00112220005
              and the Sony Research Award Program
            </p>
          </div>

          <div class="section">
            <hr />
            <p>
              Last updated on 15-Dec-2024 | Template borrowed from
              <a href="https://dex-ycb.github.io" target="_blank">DexYCB</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
