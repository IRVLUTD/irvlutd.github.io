<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="description"
      content="HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object Interaction."
    />
    <meta
      name="keywords"
      content="HO-Cap; 3D Reconstruction; Hand Pose; Object Pose; Pose Tracking; Hand-Object Interaction; Dataset; Deep Learning; 3D Vision; Computer Vision;"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Jikai Wang" />
    <title>
      HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object
      Interaction
    </title>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css"
    />
    <link rel="stylesheet" href="assets/css/style.css" />
  </head>

  <body id="page-top">
    <div class="section container-fluid text-justify text-center bg-light">
      <!-- Title -->
      <div id="title">
        <h1>
          HO-Cap: A Capture System and Dataset for <br />
          3D Reconstruction and Pose Tracking <br />
          of Hand-Object Interaction
        </h1>
      </div>
      <!-- Authors -->
      <div id="authors" class="content d-block">
        <div class="d-inline-flex gap-4 flex-wrap justify-content-center">
          <span class="nowrap"
            ><a href="https://jwroboticsvision.github.io" target="_blank">Jikai Wang</a
            ><sup>1</sup></span
          >
          <span class="nowrap"
            ><a href="https://qfantastic.github.io" target="_blank">Qifan Zhang</a
            ><sup>1</sup></span
          >
          <span class="nowrap"
            ><a href="https://research.nvidia.com/person/yu-wei-chao" target="_blank">Yu-Wei Chao</a
            ><sup>2</sup></span
          >
          <span class="nowrap"
            ><a href="https://research.nvidia.com/person/bowen-wen" target="_blank">Bowen Wen</a
            ><sup>2</sup></span
          >
          <span class="nowrap"
            ><a href="https://profiles.utdallas.edu/xguo" target="_blank">Xiaohu Guo</a
            ><sup>1</sup></span
          >
          <span class="nowrap"
            ><a href="https://yuxng.github.io" target="_blank">Yu Xiang</a><sup>1</sup></span
          >
        </div>
        <br />
        <div class="gap-2 flex-wrap justify-content-center">
          <span class="nowrap"><sup>1</sup>University of Texas at Dallas</span>
          <span class="nowrap"><sup>2</sup>NVIDIA</span>
        </div>
      </div>

      <!-- Navigation Buttons -->
      <div
        id="nav_buttons"
        class="content d-inline-flex gap-2 d-block justify-content-center flex-wrap"
      >
        <a class="btn btn-dark btn-sm" href="https://arxiv.org/abs/2406.06843" target="_blank"
          ><i class="ai ai-arxiv"></i> arXiv</a
        >
        <a class="btn btn-dark btn-sm" href="#video"><i class="fa-solid fa-film"></i> Video</a>
        <a class="btn btn-dark btn-sm" href="#code"><i class="fab fa-github"></i> Code</a>
        <a class="btn btn-dark btn-sm" href="#data"
          ><i class="fa-solid fa-cloud-arrow-down"></i> Dataset</a
        >
        <a class="btn btn-dark btn-sm" href="#cite"><i class="fas fa-quote-right"></i> Citation</a>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-md-auto">
          <div class="section center" id="abstract">
            <h2>Abstract</h2>
            <p style="text-align: justify">
              We introduce a data capture system and a new dataset, HO-Cap, for 3D reconstruction
              and pose tracking of hands and objects in videos. The system leverages multiple RGB-D
              cameras and a HoloLens headset for data collection, avoiding the use of expensive 3D
              scanners or mocap systems. We propose a semi-automatic method for annotating the shape
              and pose of hands and objects in the collected videos, significantly reducing the
              annotation time compared to manual labeling. With this system, we captured a video
              dataset of humans interacting with objects to perform various tasks, including simple
              pick-and-place actions, handovers between hands, and using objects according to their
              affordance, which can serve as human demonstrations for research in embodied AI and
              robot manipulation. Our data capture setup and annotation framework will be available
              for the community to use in reconstructing 3D shapes of objects and human hands and
              tracking their poses in videos.
            </p>
          </div>

          <!-- Video Play -->
          <div id="video" class="text-justify text-center">
            <h3>Video</h3>
            <div class="gap-2 d-md-block text-justify text-center">
              <button
                type="button"
                class="btn btn-outline-primary active video-btn"
                data-src="assets/videos/demo_video_task1_small.mp4"
              >
                Task 1: Pick-and-Place
              </button>
              <button
                type="button"
                class="btn btn-outline-primary video-btn"
                data-src="assets/videos/demo_video_task2_small.mp4"
              >
                Task 2: Handover
              </button>
              <button
                type="button"
                class="btn btn-outline-primary video-btn"
                data-src="assets/videos/demo_video_task3_small.mp4"
              >
                Task 3: Affordance Usage
              </button>
              <button
                type="button"
                class="btn btn-outline-primary video-btn"
                data-src="assets/videos/demo_video_isaacsim_small.mp4"
              >
                Isaac Sim Replay
              </button>
            </div>

            <div class="video section embed-responsive embed-responsive-4by3">
              <video id="videoPlayer" autoplay muted loop playsinline>
                <source
                  id="videoSource"
                  src="assets/videos/demo_video_task1_small.mp4"
                  type="video/mp4"
                />
                Your browser does not support the video tag.
              </video>
            </div>
            <p>
              Visualization of all the sequences could be found in
              <a
                href="https://utdallas.box.com/s/4jf5cfodt7ob1arlff0zoix02mb7srov"
                target="_blank"
                rel="noopener noreferrer"
                ><span class="icon"><i class="fas fa-download"></i> Sequence_Renderings</span></a
              >.
            </p>
          </div>

          <div id="overview" class="section">
            <h2 class="text-justify text-center">Overview</h2>
            <h3>Data Capture Setup</h3>
            <figure class="pipeline">
              <img src="./assets/images/hardware.png" alt="Data Capture Setup" />
              <figcaption>8 RealSense Cameras + HoloLens, No Mo-cap</figcaption>
            </figure>

            <h3>Object Shape Reconstruction</h3>
            <figure class="pipeline">
              <img src="./assets/images/object_shape.png" alt="Object Shape" />
              <figcaption>Recorded with single Azure Kinect Camera</figcaption>
            </figure>

            <br />
            <figure class="pipeline">
              <img src="./assets/images/object-shapes.png" alt="All Object Shapes" />
              <figcaption>All Object Shapes in HO-Cap Dataset</figcaption>
            </figure>

            <h3>Annotation Pipeline (Semi-Automatic)</h3>
            <figure class="pipeline">
              <img src="./assets/images/pipeline.png" alt="Pipeline Image" />
              <figcaption style="text-align: justify">
                The only human annotation required is to (1) manually prompt two points for each
                object in the initial frame to generate an initial segmentation mask of the object
                using SAM2, and (2) label the name of the object to associate it to an object in our
                database.
              </figcaption>
            </figure>
          </div>

          <div class="section" id="cite">
            <h3>Citation</h3>
            <p>Please cite HO-Cap if it helps your research:</p>
            <pre><code>@misc{wang2024hocapcapturedataset3d,
      title={HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object Interaction},
      author={Jikai Wang and Qifan Zhang and Yu-Wei Chao and Bowen Wen and Xiaohu Guo and Yu Xiang},
      year={2024},
      eprint={2406.06843},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.06843},
}</code></pre>
          </div>

          <div class="section" id="code">
            <h3>Code</h3>
            <p>
              <span
                ><i class="fa-brands fa-github fa-lg"></i>
                <a href="https://github.com/IRVLUTD/HO-Cap" target="_blank">HOCap-Toolkit</a></span
              >
              <br />
              <span
                >A Python package that provides evaluation and visualization tools for the HO-Cap
                dataset.</span
              >
            </p>
            <p>
              <span
                ><i class="fa-brands fa-github fa-lg"></i>
                <a href="https://github.com/IRVLUTD/HO-Cap-Annotation" target="_blank"
                  >HOCap-Annotation</a
                ></span
              >
              <br />
              <span
                >A Python package that provides hand-object poses annotations for HO-Cap
                dataset.</span
              >
            </p>
          </div>

          <div class="section" id="data">
            <h3>HO-Cap Dataset</h3>
            <p>
              HO-Cap is licensed under
              <span class="underline"
                ><a
                  href="https://creativecommons.org/licenses/by/4.0/"
                  target="_blank"
                  rel="noopener noreferrer"
                  >CC BY 4.0</a
                ></span
              >.
            </p>

            <ol>
              <li><b>Object Information</b></li>
              <ul>
                <li>
                  Object descriptions and purchase links:
                  <a
                    href="https://utdallas.box.com/s/jrj1s2wn46cv4gvpcstbhuus15sjrw7y"
                    target="_blank"
                    rel="noopener noreferrer"
                    ><span class="icon"><i class="fas fa-download"></i>Objects_Info</span></a
                  >.
                </li>
                <li>
                  Recordings for object reconstruction:
                  <a
                    href="https://utdallas.box.com/s/w1yw5ucjejpc9uijk0l1catswfg1b3g2"
                    target="_blank"
                    rel="noopener noreferrer"
                    ><span class="icon"><i class="fas fa-download"></i></span> Objects_Collection</a
                  >.
                </li>
              </ul>

              <li><b>Dataset Download</b></li>
              <ul>
                <li>
                  <b>Option One:</b> download the data with script provided by
                  <a
                    href="https://github.com/IRVLUTD/HO-Cap?tab=readme-ov-file#download-the-hocap-dataset"
                    target="_blank"
                    rel="noopener noreferrer"
                    >HOCap-Toolkit</a
                  >.
                </li>
                <li>
                  <b>Option Two:</b> download the individual zipped data from Box manually:
                  <ul>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/nlp4c6vtd0n8o0entxlh1vxdpcdeh0h8"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> calibration.zip
                        (19.6 KB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/con44iqej33weg9f3rpxof61eh3x2x21"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> models.zip (52.5
                        MB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/w0voy9bixtxyclo52841xyamock2lxpt"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_1.zip
                        (8.0 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/j498kxxrkvaf674tvmt4su4ad0bz9s9f"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_2.zip
                        (20.6 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/shklq33yaoozh9gm681nxwnq0o3y0y1d"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_3.zip (18
                        GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/dew68k7b3ya09t40818gpfxm95oa4yeq"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_4.zip
                        (8.3 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/mutor2a09kudze1yw173gsfetsru7ces"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_5.zip
                        (3.2 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/iyja7rdbjx2ksgjhmdu6mx3zqvaurdni"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_6.zip
                        (14.9 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/4g5qyig6i4uz1rgrzkcu9n4mhdjs74m2"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_7.zip
                        (9.6 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/khrb5guy8rdwnoqi4euk2w0mk5lslxkn"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_8.zip
                        (6.2 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/3x5yitydmbmwolq9bty5dd2udu5v52fc"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> subject_9.zip
                        (11.7 GB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/2lofbp2yd005d8o213ns77mdrtxg8eep"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> poses.zip (23.8
                        MB)</a
                      >
                    </li>
                    <li>
                      <a
                        href="https://utdallas.box.com/s/ayd4st2wo588z2yqbuxalptxnz2qxlj5"
                        target="_blank"
                        rel="noopener noreferrer"
                        ><span class="icon"><i class="fas fa-download"></i></span> labels.zip (1.5
                        GB)</a
                      >
                    </li>
                  </ul>
                </li>
              </ul>
            </ol>

            Once you successfully download the zip files, extract them to the "./datasets/HO-Cap"
            folder, the directory structure should look like the following:
            <pre>
<code>datasets/HO-Cap
  ├── calibration
  ├── models
  ├── subject_1
  │   ├── 20231025_165502
  │   │   ├── 037522251142
  │   │   │   ├── color_000000.jpg
  │   │   │   ├── depth_000000.png
  │   │   │   ├── label_000000.npz
  │   │   │   └── ...
  │   │   ├── 043422252387
  │   │   ├── ...
  │   │   ├── hololens_kv5h72
  │   │   ├── meta.yaml
  │   │   ├── poses_m.npy
  │   │   ├── poses_o.npy
  │   │   └── poses_pv.npy
  │   ├── 20231025_165502
  │   └── ...
  ├── ...
  └── subject_9</code>
                </pre>

            <p>
              For instructions about using the dataset please see
              <span
                ><a
                  href="https://github.com/IRVLUTD/HO-Cap"
                  target="_blank"
                  rel="noopener noreferrer"
                  >HOCap-Toolkit</a
                ></span
              >.
            </p>
          </div>

          <div class="section" id="contact">
            <h3>Contact</h3>
            <p>
              For any comments or questions, please contact Jikai Wang:
              <br />
              <a href="mailto:jikai.wang@utdallas.edu">jikai.wang@utdallas.edu</a>.
            </p>
          </div>

          <div class="section" id="acknowledgements">
            <h3>Acknowledgements</h3>
            <p>
              This work was supported in part by the DARPA Perceptually-enabled Task Guidance (PTG)
              Program under contract number HR00112220005, the Sony Research Award Program, and the
              National Science Foundation (NSF) under Grant No. 2346528.
            </p>
          </div>

          <div class="section">
            <footer class="mt-auto py-3 my-4 border-top">
              <p class="text-center text-body-secondary">Last updated on 20-Mar-2025.</p>
            </footer>
          </div>

          <!-- Back to Top Button -->
          <button id="back-to-top" class="btn btn-primary">
            <i class="fas fa-arrow-up"></i>
          </button>
        </div>
      </div>
    </div>

    <script>
      // Back to Top Button Functionality
      const backToTopButton = document.getElementById("back-to-top");

      window.addEventListener("scroll", () => {
        if (window.scrollY > 300) {
          backToTopButton.style.display = "block";
        } else {
          backToTopButton.style.display = "none";
        }
      });

      backToTopButton.addEventListener("click", () => {
        // Scroll smoothly to the Abstract section
        document.getElementById("title").scrollIntoView({ behavior: "smooth" });
        // window.scrollTo({ top: 0, behavior: "smooth" });
      });

      // Video Player
      document.addEventListener("DOMContentLoaded", function () {
        let videoPlayer = document.getElementById("videoPlayer");
        let videoSource = document.getElementById("videoSource");
        let buttons = document.querySelectorAll(".video-btn");

        buttons.forEach((button) => {
          button.addEventListener("click", function () {
            let videoSrc = this.getAttribute("data-src");
            videoSource.src = videoSrc;
            videoPlayer.load();
            videoPlayer.play();

            // Remove active class from all buttons and add to the clicked one
            buttons.forEach((btn) => btn.classList.remove("active"));
            this.classList.add("active");
          });
        });
      });
    </script>
  </body>
</html>
