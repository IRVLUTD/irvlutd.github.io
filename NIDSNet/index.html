<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="NIDS-Net, IRVLUTD, IRVL, Intelligent Robotics and Vision Lab, The University of Texas at Dallas, NVIDIA, Robot Perception, Object Recognition, NIDS-Net, Instance Detection, Instance Segmentation, Novel Instance">
  <meta name="description" content="Adapting Pre-Trained Vision Models for Novel Instance Detection and Segmentation">
  <meta name="author" content="Yangxiao Lu,  Jishnu Jaykumar P, Yunhui Guo, Nicholas Ruozzi, Yu Xiang">
  <title>NIDS-Net</title>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-CFGPJHLM4C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CFGPJHLM4C');
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./assets/css/bulma.min.css">
  <link rel="stylesheet" href="./assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./assets/css/index.css">
  <link rel="icon" href="#">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./assets/js/fontawesome.all.min.js"></script>
  <script src="./assets/js/bulma-carousel.min.js"></script>
  <script src="./assets/js/bulma-slider.min.js"></script>
  <script src="./assets/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body pad-zero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title"> <span class="small-caps">Adapting Pre-Trained Vision Models for Novel Instance Detection and Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://youngsean.github.io/">Yangxiao Lu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
            <span class="author-block">
              <a target="_blank" href="https://jishnujayakumar.github.io/">Jishnu Jaykumar P</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
            <span class="author-block">
                <a target="_blank" href="https://yunhuiguo.github.io/">Yunhui Guo</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a target="_blank" href="https://personal.utdallas.edu/~nicholas.ruozzi/">Nicholas Ruozzi</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
              <a target="_blank" href="https://yuxng.github.io">Yu Xiang</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a target="_blank" href="https://labs.utdallas.edu/irvl">The University of Texas at Dallas</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<!--            <span class="author-block"><sup>2</sup><a target="_blank" href="https://about.meta.com/">Meta</a></span>-->
            <!-- <span id="accepted-conference" class="author-block"><br>In arXiv</span> -->
            <br><span id="accepted-conference" class="author-block"><br>arXiv, 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Arxiv Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2405.17859"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> 


              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/YoungSean/NIDS-Net"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>


              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://utdallas.box.com/s/ieo7lochg1dzzdjfqm7saiudaeptufoi"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Detection Data</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://utdallas.box.com/s/yw8oazutnp1ektcnzh3hm8u5vjtq7to7"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>BOP Segmentation Data</span>
                  </a>
              </span>


            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

      <!-- Paper video. -->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column">-->
<!--          <div class="publication-video">-->
<!--&lt;!&ndash;            <iframe width="560" height="315" src="https://www.youtube.com/embed/Hf1dJ-FqGFQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>&ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/CRAVObWhxLw?si=IvSbS4sgEqou8dH6"-->
<!--            frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--&lt;!&ndash;            <iframe width="560" height="315" src="https://www.youtube.com/embed/aFTrF7NIihI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>&ndash;&gt;-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
      <!--/ Paper video. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Novel Instance Detection and Segmentation (NIDS) aims at detecting and segmenting novel object instances given a few examples of each instance. We propose a unified framework (NIDS-Net) comprising object proposal generation, embedding creation for both instance templates and proposal regions, and embedding matching for instance label assignment. Leveraging recent advancements in large vision methods, we utilize the Grounding DINO and Segment Anything Model (SAM) to obtain object proposals with accurate bounding boxes and masks. Central to our approach is the generation of high-quality instance embeddings. We utilize foreground feature averages of patch embeddings from the DINOv2 ViT backbone, followed by refinement through a weight adapter mechanism that we introduce. We show experimentally that our weight adapter can adjust the embeddings locally within their feature space and effectively limit overfitting. This methodology enables a straightforward matching strategy, resulting in significant performance gains. Our framework surpasses current state-of-the-art methods, demonstrating notable improvements of 22.3, 46.2, 10.3, and 24.0 in average precision (AP) across four detection datasets. In instance segmentation tasks on seven core datasets of the BOP challenge, our method outperforms the top RGB methods by 3.6 AP and remains competitive with the best RGB-D method.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section id="framework" class="section">
  <div class="container is-max-desktop">
    <!-- 20xScenes. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">NIDS-Net</h2>
        <p>NIDS-Net is a unified framework for Novel Instance Detection and Segmentation (NIDS).</p>
        <div class="content has-text-justified">
          <img src="assets/images/fw0.png" alt="gto">
        </div>
      </div>
    </div>
    <!--/ 20xScenes. -->
  </div>
</section>

<section id="ffa" class="section">
  <div class="container is-max-desktop">
    <!-- 20xScenes. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Foreground Feature Averaging (FFA)</h2>
        <p>FFA<a href="#bib-1"> [1] </a> is used to generate initial instance embeddings.</p>
        <div class="content has-text-justified">
          <img src="assets/images/FFA3.png" alt="gto">
        </div>
      </div>
    </div>
    <!--/ 20xScenes. -->
  </div>
</section>



<section id="detection" class="section">
  <div class="container is-max-desktop">
    <!-- 20xScenes. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Detection Examples</h2>
        <div class="content has-text-justified">
          <img src="assets/images/det6.png" alt="gto">
        </div>
      </div>
    </div>
    <!--/ 20xScenes. -->
  </div>
</section>

<section id="BOP" class="section">
  <div class="container is-max-desktop">
    <!-- 20xScenes. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">BOP Benchmark</h2>
        <h3><a href="https://bop.felk.cvut.cz/leaderboards/segmentation-unseen-bop23/core-datasets/">Ranked #1: Model-based 2D segmentation of unseen objects – Core datasets</a></h3>
        <div class="content has-text-justified">
          <img src="assets/images/leaderboard.png" alt="gto">
        </div>
      </div>
    </div>
    <!--/ 20xScenes. -->
  </div>
</section>

<section id="segmentation" class="section">
  <div class="container is-max-desktop">
    <!-- 20xScenes. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Segmentation Examples of BOP Benchmark</h2>
        <div class="content has-text-justified">
          <img src="assets/images/seg_1.png" alt="gto">
        </div>
      </div>
    </div>
    <!--/ 20xScenes. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

        <!-- Code. -->
        <div class="columns is-centered has-text-justified">
          <div class="column">
            <h2 id="code" class="title is-3">Code</h2>
            <!-- Toolkit Code. -->
            <div class="content has-text-justified">
              <h2 class="title is-3">
                <span class="link-block">
                  <a target="_blank" href="https://github.com/YoungSean/NIDS-Net"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span> <span class="small-caps">NIDS-Net</span>
                    </a>
                </span>
              </h2>
              <p>
                The code for NIDS-Net.
              </p>
            </div>

          </div>
        </div>
        <!--/ Code. -->


  </div>
</section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- References. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">References</h2>
          <div class="content has-text-justified">
            <ol id="references">
                <li id="bib-1">
                    <p>
                    K. Kotar, S. Tian, H.-X. Yu, D. Yamins, and J. Wu. Are these the same apple? comparing images based on object intrinsics. Advances in Neural Information Processing Systems, 36, 2024.<a href="https://arxiv.org/abs/2311.00750"> arXiv </a>
                    </p>
                </li>
            </ol>
          </div>
        </div>
      </div>
      <!--/ References. -->
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">

        <!-- Code. -->
        <div class="columns is-centered has-text-justified">
          <div class="column">
            <h2 id="data" class="title is-3">All Embeddings, Model Weights and Predictions</h2>
            <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://utdallas.box.com/s/ieo7lochg1dzzdjfqm7saiudaeptufoi"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Detection Data</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://utdallas.box.com/s/yw8oazutnp1ektcnzh3hm8u5vjtq7to7"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>BOP Segmentation Data</span>
                  </a>
              </span>

          </div>
        </div>
  </div>
</section>
  

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">Citation (BibTeX) </h2>
    <span>Please cite <span class="small-caps">NIDS-Net</span> if it helps your research:</span>
    <pre><code>
      @misc{lu2024adapting,
      title={Adapting Pre-Trained Vision Models for Novel Instance Detection and Segmentation},
      author={Yangxiao Lu and Jishnu Jaykumar P and Yunhui Guo and Nicholas Ruozzi and Yu Xiang},
      year={2024},
      eprint={2405.17859},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
      }
    </code></pre>
  </div>
</section>

<section class="section" id="contact">
  <div class="container is-max-desktop">
    
    <h2 class="title">Contact</h2>
      <p>Send any comments or questions to Yangxiao Lu: <a target="_blank" href="mailto:yangxiao.lu@utdallas.edu">yangxiao.lu@utdallas.edu</a> </p>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
  <div class="is-vcentered interpolation-panel">
    <div class="container content">
      <p style="font-size: 16px">
        This work was supported in part by the DARPA Perceptually-enabled Task Guidance (PTG) Program under contract number HR00112220005.
      </p>
    </div>
  </div>
</div>  
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a target="_blank" href="https://arxiv.org/pdf/2211.11679.pdf" class="large-font bottom_buttons black-font">
        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
      </a>
      <br>
      <p>Last updated on 23-Mar-2023 | Page template borrowed from <a target="_blank" href="https://nerfies.github.io"><span class="small-caps black-font">Nerfies</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
